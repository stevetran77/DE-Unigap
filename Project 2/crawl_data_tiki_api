#%%
import requests
import pandas as pd
import time
from bs4 import BeautifulSoup
from pandas import json_normalize
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import re

file_path = r'C:\Users\Windows\DE-Unigap\Project 2\list_item.csv'

#%% L·∫•y danh s√°ch  product_id t·ª´ csv
def load_product_id_from_csv(file_path, limit=None, column_name='id'): # T√πy ch·ªânh s·ªë l∆∞·ª£ng product load n·∫øu load all th√¨ ƒë·ªïi limit th√†nh None
    try:
        df = pd.read_csv(file_path)
        if limit is not None:
            df = df.head(limit) 
            print(f"S·ªë l∆∞·ª£ng product_id: {df['id'].count()}")
            print('L·∫•y danh s√°ch product th√†nh c√¥ng')
        return df['id'].tolist()  # C·ªôt ph·∫£i c√≥ t√™n l√† product_id
    except FileNotFoundError:
        print("‚ùå File kh√¥ng t·ªìn t·∫°i. Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")
        return []
    except KeyError:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt '{column_name}' trong file CSV.")
        return []
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc file: {e}")
        return []

#%% K·∫øt n·ªëi v·ªõi API v√† get data v·ªÅ
def fetch_product(pid):
    url = f"https://api.tiki.vn/product-detail/api/v1/products/{pid}"
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json"
    }
    try:
        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code == 200:
            data = response.json()
            data['error'] = None
            return data
        else:
            return {"id": pid, "error": f"HTTP {response.status_code}"}
    except Exception as e:
        return {"id": pid, "error": str(e)}

def crawl_product_info(product_ids, max_workers=10):
    print(f"B·∫Øt ƒë·∫ßu Crawl {len(product_ids)} s·∫£n ph·∫©m...")

    results = []
    start_time = time.time()
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_pid = {executor.submit(fetch_product, pid): pid for pid in product_ids}
        for future in tqdm(as_completed(future_to_pid), total=len(future_to_pid), desc="üì¶ Crawling"):
            result = future.result()
            results.append(result)

    end_time = time.time()
    duration = round(end_time - start_time, 2)
    print(f"‚úÖ Crawl ho√†n t·∫•t trong {duration/60:.2f} ph√∫t)")
    print("‚úÖ Crawl xong!")
    return results


#%% Tr√≠ch ·∫£nh t·ª´ description
def extract_images_from_html(html):
    soup = BeautifulSoup(html or "", "html.parser")
    return [img['src'] for img in soup.find_all('img') if img.get('src')]

#%% Tr√≠ch ·∫£nh t·ª´ description
def standarize_description(html):
    soup = BeautifulSoup(html or "", "html.parser")
    # X√≥a ·∫£nh
    for img in soup.find_all("img"):
        img.decompose()

    for a in soup.find_all('a'):
        a.decompose()
    # L·∫•y text
    clean_text = soup.get_text(separator=' ', strip=True).replace('\n', ' ').replace('\xa0', ' ')

    clean_text = re.sub(r'\s+', ' ', clean_text)
    return clean_text

#%% Clean Data
def transform_product_data(product_list):
    prd_short_list = [{
        'id': p.get('id'),
        'name': p.get('name'),
        'url_key': p.get('url_key'),
        'description': standarize_description(p.get('description')),
        'images_url': [img.get('base_url') for img in p.get('images', []) if img.get('base_url')],
        'error': p.get('error')
    } for p in product_list]

    print(f"Cleaning Data is completed")
    return prd_short_list

#%% L·∫•y list product t·ª´ file csv

product_ids = load_product_id_from_csv(file_path, limit = None ,column_name = 'id')

#%% Crawl th√¥ng tin product  v·ªÅ
product_list = crawl_product_info(product_ids)
product_list[0]

#%% Clean data
prd_short_list = transform_product_data(product_list)

#%% Chuy·ªÉn th√†nh dataframe
prd_df = json_normalize(prd_short_list)
prd_df.head()
# prd_df.to_excel(r'C:\Users\Windows\OneDrive\Desktop\check.xlsx') # Check data

# %%
